SimulTron: On-Device Simultaneous Speech to Speech Translation
Alex Agranovich1, Eliya Nachmani1, Oleg Rybakov1, Yifan Ding2, Ye Jia2, Nadav Bar1, Heiga Zen2, Michelle Tadmor Ramanovich1
1Google Research 2Google DeepMind alexagr@google.com, eliyn@google.com
Abstract Simultaneous speech-to-speech translation (S2ST) holds the promise of breaking down communication barriers and enabling fluid conversations across languages. However, achieving ac-curate, real-time translation through mobile devices remains a major challenge. We introduce SimulTron, a novel S2ST archi-tecture designed to tackle this task. SimulTron is a lightweight direct S2ST model that uses the strengths of the Translatotron framework while incorporating key modifications for stream-ing operation, and an adjustable fixed delay. Our experiments show that SimulTron surpasses Translatotron 2 in offline eval-uations. Furthermore, real-time evaluations reveal that Simul-Tron improves upon the performance achieved by Translatotron 1. Additionally, SimulTron achieves superior BLEU scores and latency compared to previous real-time S2ST method on the MuST-C dataset. Significantly, we have successfully deployed SimulTron on a Pixel 7 Pro device, show its potential for simul-taneous S2ST on-device. Index Terms: Speech to speech translation, On-device, Real-time, Translatotron
1. Introduction Speech-to-speech translation (S2ST) is a transformative tech-nology with the potential to break down language barriers and foster global connections. In recent years, groundbreaking models [1, 2, 3, 4, 5] have revolutionized the field of S2ST. These models have achieved remarkable performance, surpass-ing the traditional cascade-based approach with their direct S2ST translation methods. Moreover, they preserve speaker identity, intonation, and other subtle nuances that lend authen-ticity and expressiveness to speech. While S2ST technology continues to evolve, the challenge of real-time, on-device si-multaneous translation persists. Existing simultaneous transla-tion models [6, 7, 8, 5, 9, 10] are not optimized for the inherent constraints of mobile devices. Today, with smartphones and tablets being central hubs for personal and professional interac-tions, on-device S2ST is crucial. This approach offers increased accessibility, privacy, and the ability to bridge linguistic divides.
We introduce SimulTron, a novel on-device, simultaneous S2ST model built upon the foundation of the Translatotron ar-chitecture [1]. SimulTron leverages the strengths of Transla-totron while incorporating key modifications, including a causal conformer encoder, wait-k attention, convolutional post-net net-work, and streaming vocoder specifically tailored for the on-device, simultaneous translation scenario. Our contributions in this paper are threefold:
• Real-time On-Device Simultaneous S2ST: We present on-
device, simultaneous S2ST model, the model starts out-putting the translation after an adjustable fixed delay for con-
text. Paving the way for real-time, language-agnostic com-munication on mobile devices. SimulTron processes 320-sample audio packets in streaming mode. Features are ex-tracted via mel spectrograms and encoded by a 16-layer causal conformer encoder. The encoded output is decoded in streaming mode and synthesized into audio by a streaming vocoder, enabling instant translation output during speech.
• Enhanced Translatotron Architecture: We propose im-provements to the Translatotron architecture, leading to su-perior performance for both offline and simultaneous S2ST. Notably, our model demonstrates a clear improvement over the offline Translatotron architecture in simultaneous set-tings and surpasses the Translatotron 2 architecture in of-fline evaluations. Additionally, SimulTron achieves supe-rior BLEU scores and latency compared to previous real-time S2ST methods on the MuST-C dataset. This demonstrates that SimulTron can effectively translate speech while pre-serving its natural characteristics, even under the constraints of on-device processing.
• Comprehensive Analysis: We conduct a rigorous evalua-tion of SimulTron, examining its performance across vari-ous floating-point precision levels and latency metrics. We also compare SimulTron’s performance against offline mod-els, providing valuable insights into its efficiency and effec-tiveness. This analysis sheds light on the trade-offs inherent in on-device S2ST translation.
2. Related Work 2.1. Offline speech-to-speech translation
Speech-to-speech translation has revolutionized cross-language communication, evolving from modular systems to end-to-end models. Traditionally, S2ST relied on a cascade of speech-to-text, machine translation, and text-to-speech systems [11, 12, 13]. However, the field has seen a significant shift towards end-to-end training models, offering increased efficiency and accuracy. The Translatotron [1] architecture pioneered direct S2ST, preserving speaker identity and using phoneme recogni-tion loss, but initially lagged behind cascade systems in perfor-mance. Translatotron 2 [14] enhanced the original model with a linguistic decoder and a unique method for retaining speaker voice, significantly boosting its performance to rival cascade systems. Translatotron 3 [15] utilizes unsupervised learning with back-translation, enabling training on readily available speech data without requiring paired translations. Underscoring the importance of non-linguistic cues, Translatotron 3 uniquely emphasizes the preservation of non-linguistic cues in speech. Other notable works in unsupervised S2ST include [16, 17], which achieve comparable results to supervised methods. An-
Streaming mel frontend
Streaming Conformer Encoder
Streaming WaitK
attention
2 LSTMLinear projection
Streaming PostNet: 5 Conv
Streaming vocoder
PreNet
Input audio
Output audio
Linear projection
Stop token
Decoder
Target auxiliary decoder
Source auxiliary decoder
Auxiliary recognition tasks
Figure 1: An overview of the proposed SimulTorn architecture. First, the streaming encoder generates a compact representa-tion of the source language input. Subsequently, the decoder, employing wait-k attention, produces a mel-spectrogram rep-resentation of the target translation. The MelGAN vocoder then synthesizes the final translated speech output from the mel-spectrogram.
other significant development in S2ST involves using speech to-kens for training models. Notable examples include UWSpeech [2], Speech-to-Unit Translation (S2UT) [3, 4], and Transpeech [5]. This approach is crucial because speech token-based S2ST models can be trained on unwritten languages, expanding the potential of S2ST. Furthermore, recent S2ST research has ex-plored the integration of audio tokens [18, 19, 20, 21, 22, 23] with large language models (LLMs) [24, 25, 26, 27, 28]. This combination offers the potential to improve translation quality and fluency by leveraging the vast linguistic knowledge embed-ded within LLMs.
2.2. Real time speech-to-speech translation
Various architectures have been explored to address the chal-lenges of real-time speech-to-speech translation. A common ap-proach involves cascading automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) systems [6]. However, this can introduce additional latency. To mitigate latency and improve translation quality, direct S2ST models have been developed. For instance, Ma et al. (2021) proposed a simultaneous S2ST system employing discrete units and a novel variational monotonic multihead attention (V-MMA) mecha-nism to refine policy learning [7]. Similarly, Liu et al. (2021) focused on latency reduction by combining upstream speech translation with incremental text-to-speech (iTTS) [8]. Non-autoregressive methods have gained traction for their potential speed gains. The TranSpeech architecture [5] leverages bilateral perturbation, style normalization, and information enhancement for non-autoregressive S2ST, demonstrating significant speed improvements over autoregressive models. Further advance-ments are evident in the Seamless architecture [9], where ef-ficient monotonic multihead attention (EMMA) minimizes la-tency while maintaining the speaker’s vocal style and prosody. Notably, the SimulS2ST architecture [10] offers simultaneous S2ST for a 57 source languages into English, with the flexibil-ity to adjust latency parameters. Importantly, SimulTron estab-lishes a milestone as the first method to demonstrate real-time S2ST on a device.
3. Model Architecture The Simultron architecture comprises three models based on the Translatotron 1 architecture tailored for streaming operation: Streaming Encoder: responsible for the real-time encoding of the source language input audio signal. Its design focuses on generating a compact and informative representation of the tar-get language, while maintaining low latency to ensure a respon-sive system. Streaming Decoder: The autoregressive LSTM decoder uses a wait-k attention to processes the encoded rep-resentation from the encoder in sequential fashion, producing acoustic features (mel spectrograms) frame-by-frame. Stream-ing Vocoder: The final stage in the SimulTron pipeline involves a streaming vocoder that is responsible for converting the gen-erated acoustic features into a time-domain audio waveform. Overview of the proposed SimulTorn architecture is present at Figure 1.
3.1. Streaming Encoder
Input audio frames are processed in streaming mode by mel frontend (Figure 1) with 80 mel bins, audio framing is done with frame size 25ms and frames step 10ms. Streaming en-coder (Figure 1) receives two mel frames (every frame with 80 bins) and process them in streaming mode by 16 Conformer blocks and one 2x subsampling layer (because of subsampling layer encoder needs two input frames). Streaming conformer block is composed of a sequence of layers: projection layer fol-lowed by causal self attention layer with local context looking into 65 frames in the past (also called left context), followed by 1d depth-wise convolution layer with kernel size 32 followed by projection layer and layer normalization. Every layer in the conformer block has residual connection except last layer nor-malization. Causal self attention layer uses 8 heads with 256 feature dimension. All layers in this pipeline: mel frontend, encoder with self attention and convolution layers are causal. Conformer block is open sourced at [29].
3.2. Streaming Decoder and Vocoder
Figure 1 show the spectrogram decoder. It has similar structure with the decoder of the Translatotron model which consists of an attention block, a pre-net, an autoregressive LSTM stack, and PostNet components. We make several changes to it in order to enable causal inference. We use wait-k attention [30] instead of multi-headed attention. This mechanism allows the decoder to attend to the ’k’ most recently frames of the encoded source sequence, calculating attention weights accordingly. Given the real-time constraints of SimulTron, an initialization step is re-quired where a buffer accumulates ’k’ frames from the encoded source sequence. This accumulation introduces a necessary de-lay, the duration of which is directly determined by the value of ’k’. Importantly, ’k’ represents frames of the encoded source sequence, where each encoded frame encapsulates a 20ms seg-ment of the original audio input. Following the wait-k attention, and the LSTM decoder Simultron replaces the PostNet with a causal convolutional PostNet [31]. Output frames of stream-ing decoder are sequentially passed to a streaming-capable Mel-GAN vocoder[32], which is responsible for synthesizing the fi-nal time-domain waveform representation of the audio.
3.3. Real-time Inference
To optimize real-time inference and minimize latency, the Si-mulTron inference architecture employs parallelization between the encoder and decoder components. The system utilizes two
distinct threads: one dedicated to the encoder and another for the combined decoder-vocoder module. This concurrent execu-tion strategy enables the encoder to process an impending frame while the decoder and vocoder operate on the current frame. Moreover, alternative model configurations featuring reduced model sizes facilitate the potential for single-threaded execution of both encoder and decoder-vocoder components. The prereq-uisite for this mode of operation is that the combined compu-tational latency incurred by both components must not exceed the duration of a single output frame. This optimization has sig-nificant implications for resource-constrained deployment sce-narios. To facilitate real-time inference on resource-constrained mobile devices, SimulTron leverages TFlite (TensorFlow Lite) models [33]. TFlite is a specialized machine learning frame-work designed for on-device deployment, offering several key advantages. Firstly, TFlite models boast a significantly smaller memory footprint compared to standard TensorFlow models, making them exceptionally well-suited for the memory limi-tations of mobile environments. Secondly, TFlite models are optimized for accelerated execution, ensuring both low latency and efficient utilization of mobile device hardware. These char-acteristics are crucial for enabling the smooth and responsive operation of SimulTron in real-world mobile applications.
4. Experiments and Results We conduct experiments utilizing the Conversational dataset [34] for English-Spanish translation tasks. The model is trained specifically for Spanish-to-English translation. The Conversa-tional dataset offers a substantial 979k utterance pairs, with 1,400 hours of Spanish speech data and 619 hours of English speech data. The Spanish audio is sampled at 16kHz, while the English audio is sampled at 24kHz. For our Spanish-to-English translation experiments, we employed the MuST-C dataset [35]. This dataset comprises 504 hours of English TED Talks audio sampled at 16kHz. The Spanish component was generated via a proprietary text-to-speech (TTS) system with a female voice and a 24kHz sampling rate. For a full overview of the hyper-parameters used, please refer to Table 5. The Adam optimizer [36] was employed for the training of SimulTron. For audio ex-amples, please refer to the project website. In the mean opinion score (MOS) experiment, we assess the naturalness of the trans-lated speech using a standard 5-point scale with human evalua-tors. Our latency metric is calculated in the same manner as in iTTS [8].
4.1. Results
4.1.1. Conversational dataset
Translation and Acoustics evaluation: Table 1 compares real-time SimulTron, offline SimulTron, and prior methods on the Conversational Spanish-to-English dataset. The SimulTron real-time model with a delay of 3sec (k = 150) exhibits per-formance improvement of 0.8 BLEU point compare to Trans-latotron 1 despite being a real-time model, attaining a BLEU score of 51.2 points. This result stems primarily from the shift to a conformer encoder architecture and the use of a 128 mel spectrogram target representation. Importantly, reducing the initial delay to k = 100 and k = 50 (equal to 2sec and 1sec delay) degrades performance 1.2 and 3.4 BLEU points respec-tively, underscoring the importance of sufficient input context for the decoder’s accuracy. We also evaluate Mean Opinion Scores (MOS) for SimulTron with varying k values (150, 100, 50) and observe a decline in audio quality with shorter input
Table 1: Performance of SimulTron the Conversational Spanish-English dataset and comparison to previous method.
Mode Model BLEU (↑) MOS (↑)
Offline
Translatotron 1 50.4 4.15 ± 0.07 Translatotron 2 55.6 4.21 ± 0.06 Cascade (ST ↔ TTS) 58.8 4.31 ± 0.06 Reference (synthetic) 81.9 3.37 ± 0.09 SimulTron 57.4 3.58 ± 0.08
Realtime SimulTron (k=50) 47.8 3.04 ± 0.09 SimulTron (k=100) 50.0 3.14 ± 0.09 SimulTron (k=150) 51.2 3.35 ± 0.09
Table 2: Performance of SimulTron the MuST-C English-Spanish dataset and comparison to previous method.
Mode Model BLEU (↑) Latency (s) (↓)
Realtime iTTS [8] 13.5 1.8 SimulTron (k=125) 14.6 1.1 SimulTron (k=150) 14.7 2.3
contexts. Additionally, the performance gap compared to of-fline models emphasizes the need for high-fidelity streaming vocoder development. We evaluate an offline variant of Si-mulTron, replacing the streaming conformer encoder with a non-streaming variant and employing a BLSTM decoder with multi-head attention (aligning with Translatotron 1). Offline Si-mulTron demonstrates a 6.2 BLEU point improvement over its real-time counterpart, attributed to its ability to leverage full-sentence context at each decoding step, whereas real-time Si-mulTron is constrained by causal limitations. Moreover, offline SimulTron outperforms Translatotron 1 and 2 by 7.0 and 1.8 BLEU points respectively. However, it shows a MOS degrada-tion of 0.57 points compared to Translatotron 1, which could be addressed with a stronger vocoder. The BLEU gain stems pri-marily from the shift to a conformer encoder architecture and the use of a 128 mel spectrogram target representation. While slightly trailing the Cascade approach by 1.4 BLEU points, this performance remains notable.
Real-time performance evaluation: Figure 2 presents real-time performance analysis of SimulTron on a Pixel 7 Pro Android device. For streaming translation, the real-time fac-tor (RTF), defined as the ratio of speech output frame length to frame processing time, must exceed 1. We examine how model configuration and execution mode impact single-frame latency. The x-axis denotes model components (encoder, decoder-vocoder) and their execution mode (concurrent or se-quential). Concurrent execution leverages multi-threading, en-abling the decoder to process a previous frame while the en-coder works on the current one, potentially enhancing effi-ciency. Sequential execution involves running both compo-nents within the same thread. Model variations with differ-ing LSTM decoder dimensionality are represented by colored lines (encoder remains unchanged). As anticipated, models with smaller decoder LSTM dimensions and fewer layers con-sistently demonstrate lower latencies. Crucially, in concurrent execution, all model configurations exhibit RTFs significantly exceeding 1, with margins of 7ms or more, demonstrating real-time translation. With sequential execution, all but the largest model (768/6) also demonstrate feasibility for the tested de-
Figure 2: The latency attributed to each model components (En-coder, Decoder+Vocoder), is assessed, with a delineation at the 25-millisecond threshold denoting the practical real-time oper-ational limit.
vice. Resource performance: Device limitations, particularly
memory and storage constraints, are critical considerations for on-device deployment of translation models. Table 3 presents memory footprints, RTFs, latencies, and BLEU scores for var-ious SimulTron configurations (differing in LSTM decoder di-mension and layer count). As expected, smaller models exhibit reduced memory and storage requirements, making them more suitable for resource-constrained mobile devices. For instance, the 768/6 model necessitates 562 Mb of memory and 259 Mb of storage, whereas the 256/4 model functions within just 335 Mb of memory and 148 Mb of storage. However, this resource efficiency often incurs a trade-off in translation quality, as indi-cated by slightly lower BLEU scores when the decoder dimen-sion and/or the number of layers are decreased. This highlights the tension between model performance and the feasibility of on-device deployment.
Table 3: Performance summary for SimulTron models.
Decoder Dim. # Layer
768 6
512 6
256 6
768 4
512 4
256 4
Latency [ms](↓) 18 12 8 16 11 8 RTF (↑) 1.4x 2x 3.1x 1.5x 2.3x 3.1x Size [MB](↓) 259 195 154 217 175 148 Memory [MB](↓) 562 431 347 476 391 335 BLEU (↑) 51.2 50.9 49.8 51.0 50.7 49.6
Post-Training Quantization: We implement post-training dynamic range quantization to reduce model size and improve efficiency. As can be seen in Table 4 weights that are quantized to int8, dramatically decreasing latency by factor of 2 to the en-coder, 2 and 1.8 for the decoder with dimension of 1024 and 768 respectively. For model storage size dramatically decreas-ing by factor of 3.7 to the encoder, 3.4 and 3.1 for the decoder with dimension of 1024 and 768 respectively. Moreover, mem-ory footprints dramatically decreasing by factor of 4.7 to the encoder, 4.2 and 3.5 for the decoder with dimension of 1024 and 768 respectively.
Table 4: Latency in sec. 10s benchmark of non-streaming Si-mulTron on Pixel7pro using float32 and Int8 tensor operations. Lower is better.
Metric Type Encoder Decoder 1024
Decoder 768
Latency [ms] Float32 8 26 18 (↓) Int8 4 13 10
Size [MB] Float32 104 241 155 (↓) Int8 28 71 50
Memory [MB] Float32 227 509 335 (↓) Int8 48 119 94
Table 5: Table of hyper-parameters.
Hyper-parameter
Input / output sample rate (Hz) 16k / 24k Learning rate 0.001 Batch Size 1024 Encoder (layers × dim) 16×256 Decoder (layers × dim) 6×678 Decoder Attn Wait-K (Input frames) 150 Decoder Attn dropout prob 0.1 Decoder output mel channels 128 Encoder frame size [ms] 20 Decoder frame size [ms] 25 Vocoder input dim 128 Vocoder mel edges Hz 20Hz-12kHz
4.1.2. MuST-C dataset
Table 2 highlights the comparative analysis of our method against a leading real-time S2ST approach. Our method im-proves the iTTS method [8] by 1.1 and 1.2 points for k=125 and k=150, respectively. Importantly, we also demonstrate a latency reduction of 0.7 seconds compared to the iTTS model when k=125.
5. Conclusion SimulTron, a real-time S2ST architecture designed to provide real-time, on-device simultaneous speech-to-speech translation. SimulTron builds upon the Translatotron framework, incorpo-rating optimizations like a causal conformer encoder, wait-k attention, a convolutional post-net, and a streaming vocoder that enable efficient, streaming S2ST operation. SimulTron sur-passes Translatotron 2 in offline evaluations and maintains com-parable performance to Translatotron 1 in real-time Spanish-to-English translation settings. Additionally, SimulTron achieves better BLEU scores and latency compared to the iTTS real-time S2ST approach on the MuST-C dataset. Moreover, SimulTron’s careful design enables it to function within the constraints of mobile devices. By bringing real-time, simultaneous translation directly to mobile devices, we envision a future where language barriers are significantly reduced, fostering greater understand-ing and collaboration across cultures. Future work will focus on expanding SimulTron’s multilingual capabilities, further op-timizing for various mobile hardware, and exploring techniques to enhance translation quality under challenging acoustic con-ditions.
6. References [1] Y. Jia et al, “Direct speech-to-speech translation with a sequence-
to-sequence model,” Proc. Interspeech, pp. 1123–1127, 2019.
[2] C. Zhang et al, “Uwspeech: Speech to speech translation for un-written languages,” in Proc. AAAI, vol. 35, 2021, pp. 14 319– 14 327.
[3] A. Lee et al, “Direct speech-to-speech translation with discrete units,” arXiv:2107.05604, 2021.
[4] ——, “Textless speech-to-speech translation on real data,” arXiv:2112.08352, 2021.
[5] R. Huang et al, “TranSpeech: Speech-to-speech translation with bilateral perturbation,” arXiv:2205.12523, 2022.
[6] K. Sudoh, T. Kano, S. Novitasari, T. Yanagita, S. Sakti, and S. Nakamura, “Simultaneous speech-to-speech translation sys-tem with neural incremental asr, mt, and tts,” arXiv preprint arXiv:2011.04845, 2020.
[7] X. Ma, H. Gong, D. Liu, A. Lee, Y. Tang, P.-J. Chen, W.-N. Hsu, P. Koehn, and J. Pino, “Direct simultaneous speech-to-speech translation with variational monotonic multihead attention,” arXiv preprint arXiv:2110.08250, 2021.
[8] D. Liu, C. Wang, H. Gong, X. Ma, Y. Tang, and J. Pino, “From start to finish: Latency reduction strategies for incremental speech synthesis in simultaneous speech-to-speech translation,” arXiv preprint arXiv:2110.08214, 2021.
[9] L. Barrault, Y.-A. Chung, M. C. Meglioli, D. Dale, N. Dong, M. Duppenthaler, P.-A. Duquenne, B. Ellis, H. Elsahar, J. Haa-heim et al., “Seamless: Multilingual expressive and streaming speech translation,” arXiv preprint arXiv:2312.05187, 2023.
[10] L. Dugan, A. Wadhawan, K. Spence, C. Callison-Burch, M. McGuire, and V. Zordan, “Learning when to speak: Latency and quality trade-offs for simultaneous speech-to-speech transla-tion with offline models,” arXiv preprint arXiv:2306.01201, 2023.
[11] S. Nakamura et al, “The ATR multilingual speech-to-speech trans-lation system,” IEEE Trans. ASLP, vol. 14, no. 2, pp. 365–376, 2006.
[12] W. Wahlster, Verbmobil: Foundations of speech-to-speech trans-lation. Springer Science & Business Media, 2013.
[13] A. Lavie et al, “JANUS-III: Speech-to-speech translation in mul-tiple languages,” in Proc. ICASSP, vol. 1, 1997, pp. 99–102.
[14] Y. Jia et al, “Translatotron 2: High-quality direct speech-to-speech translation with voice preservation,” in Proc. ICML, 2022, pp. 10 120–10 134.
[15] E. Nachmani, A. Levkovitch, Y. Ding, C. Asawaroengchai, H. Zen, and M. T. Ramanovich, “Translatotron 3: Speech to speech translation with monolingual data,” arXiv preprint arXiv:2305.17547, 2023.
[16] C. Wang et al, “Simple and effective unsupervised speech transla-tion,” arXiv:2210.10191, 2022.
[17] Y.-K. Fu, L.-H. Tseng, J. Shi, C.-A. Li, T.-Y. Hsu, S. Watan-abe, and H.-y. Lee, “Improving cascaded unsupervised speech translation with denoising back-translation,” arXiv preprint arXiv:2305.07455, 2023.
[18] A. Tjandra, S. Sakti, and S. Nakamura, “Speech-to-speech trans-lation between untranscribed unknown languages,” in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 593–600.
[19] W.-N. Hsu et al, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans-actions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.
[20] A. Baevski et al, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” Advances in neural informa-tion processing systems, vol. 33, pp. 12 449–12 460, 2020.
[21] A. Défossez et al, “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022.
[22] Y.-A. Chung et al, “W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in 2021 IEEE Automatic Speech Recognition and Un-derstanding Workshop (ASRU). IEEE, 2021, pp. 244–250.
[23] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, “Soundstream: An end-to-end neural audio codec,” IEEE ACM Trans. Audio Speech Lang. Process., vol. 30, pp. 495–507, 2022.
[24] K. Wei et al, “Joint pre-training with speech and bilingual text for direct speech to speech translation,” in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Pro-cessing (ICASSP). IEEE, 2023, pp. 1–5.
[25] M. Kim et al, “Many-to-many spoken language translation via unified speech and text representation learning with unit-to-unit translation,” arXiv preprint arXiv:2308.01831, 2023.
[26] H. Inaguma et al, “Unity: Two-pass direct speech-to-speech translation with discrete units,” arXiv preprint arXiv:2212.08055, 2022.
[27] X. Li et al, “Textless direct speech-to-speech translation with dis-crete speech representation,” in ICASSP 2023-2023 IEEE Inter-national Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.
[28] P. K. Rubenstein et al, “Audiopalm: A large language model that can speak and listen,” arXiv preprint arXiv:2306.12925, 2023.
[29] J. Shen, P. Nguyen, Y. Wu, Z. Chen et al., “Lingvo: a modular and scalable framework for sequence-to-sequence modeling,” 2019.
[30] M. Ma, L. Huang, H. Xiong, R. Zheng, K. Liu, B. Zheng, C. Zhang, Z. He, H. Liu, X. Li et al., “Stacl: Simultaneous trans-lation with implicit anticipation and controllable latency using prefix-to-prefix framework,” arXiv preprint arXiv:1810.08398, 2018.
[31] O. Rybakov, F. Biadsy, X. Zhang, L. Jiang, P. Meadowlark, and S. Agrawal, “Streaming Parrotron for on-device speech-to-speech conversion,” in Proc. INTERSPEECH 2023, 2023, pp. 2033– 2037.
[32] O. Rybakov, M. Tagliasacchi, Y. Li, L. Jiang, X. Zhang, and F. Biadsy, “Real time spectrogram inversion on mobile phone,” in Proc. INTERSPEECH 2023, 2023, pp. 4314–4318.
[33] “TensorFlow Lite — ML for Mobile and Edge Devices — ten-sorflow.org,” https://www.tensorflow.org/lite, [Accessed 05-03-2024].
[34] Y. Jia et al, “Leveraging weakly supervised data to improve end-to-end speech-to-text translation,” in Proc. ICASSP, 2019, pp. 7180–7184.
[35] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and M. Turchi, “Must-c: A multilingual corpus for end-to-end speech translation,” Computer Speech & Language, vol. 66, p. 101155, 2021.
[36] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-mization,” arXiv preprint arXiv:1412.6980, 2014.